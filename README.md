# GorAI LLM Client

A unified LLM client library supporting multiple providers (OpenAI, Anthropic, etc.) with advanced features like tool calling, streaming responses, and automatic conversation loops.

## Version

**Current Version: 0.3.2**

## Features

- ğŸ”Œ **Multiple LLM Providers**: Support for OpenAI, Anthropic, DeepSeek, and Minimax APIs
- ğŸ”§ **Tool Calling**: Built-in support for function/tool calling with automatic execution
- ğŸŒŠ **Streaming Responses**: Real-time streaming for better user experience
- ğŸ”„ **Conversation Loops**: Automatic multi-turn conversations with `chatToNextLoop`
- ğŸ’­ **Thinking Support**: Handle model reasoning/thinking content (Anthropic extended thinking)
- ğŸ§  **Interleaved Thinking Router**: Support for DeepSeek OpenAI-compatible and Minimax Anthropic-compatible interleaved thinking interactions
- ğŸ–¼ï¸ **Image Support**: Built-in support for image format handling
- ğŸ¯ **Unified Interface**: Consistent API across different providers
- ğŸ› ï¸ **Flexible Tool Execution**: Custom tool executors with `ToolExecutor` interface

## Examples

Check the `examples/` directory for complete examples:
- `chat_with_tools_example.py`: Comprehensive examples of tool calling

## Project Structure

```
GorAI_LLMClient/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py                    # create_model factory function
â”‚   â”œâ”€â”€ _model_base.py                 # Base model class
â”‚   â”œâ”€â”€ _openai_model.py               # OpenAI implementation
â”‚   â”œâ”€â”€ _anthropic_model.py            # Anthropic implementation
â”‚   â”œâ”€â”€ _deepseek_openai_model.py      # DeepSeek OpenAI-compatible implementation
â”‚   â””â”€â”€ _minimax_anthropic_model.py    # Minimax Anthropic-compatible implementation
â”œâ”€â”€ message/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ _message_base.py      # MsgReturn message format
â”œâ”€â”€ executor.py               # Tool executor interfaces
â””â”€â”€ examples/
    â””â”€â”€ chat_with_tools_example.py
```

## License

MIT License